{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Машинное обучение\n",
    "## Домашнее задание №3 - Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 16 февраля 2023, 08:30   \n",
    "**Штраф за опоздание:** -2 балла за каждые сутки\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В бустингах будем обучаться на антиградиент $-dL(a(x_i), y_i)$\n",
    "1) $-dL(a(x_i), y_i) = 2(y_i - a(x_i))\\cdot da(x_i)$ \\\n",
    "2) $-dL(a(x_i), y_i) = exp(-a(x_i) y_i)y_i\\cdot da(x_i)$ \\\n",
    "3) $-dL(a(x_i), y_i) = \\dfrac{exp(-a(x_i) y_i)y_i\\cdot da(x_i)}{1 + exp( -a(x_i) y_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ремарка: я точно знаю основы анализа и дифференцирования, но эта шляпа ведет себя странно. Если брать антиградиент как надо, то алгоритм расходится. Если брать с минусом, то он начинает сходится, но по сути это становится градиентом. Мб я где-то чего-то не доглядел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def grad(self, y_true, y_pred):\n",
    "        return 2 * (y_true - y_pred)\n",
    " \n",
    "\n",
    "class Exponential:\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.sum(np.exp(-y_pred * y_true))\n",
    "    \n",
    "    def grad(self, y_true, y_pred):\n",
    "        return np.exp(-y_pred * y_true) * y_true\n",
    "    \n",
    "\n",
    "class Logarithmic:\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.sum(np.log(1 + np.exp(-y_pred * y_true)))\n",
    "    \n",
    "    def grad(self, y_true, y_pred):\n",
    "        return (np.exp(-y_pred * y_true) * y_true) / (1 + np.exp(-y_pred * y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, loss='MSE', learning_rate=1e-1, n_estimators=100, colsample=0.7, subsample=0.7, random_state=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        subsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        loss_dict = {\n",
    "            'mse' : MSE(),\n",
    "            'exp' : Exponential(),\n",
    "            'log' : Logarithmic()\n",
    "        }\n",
    "        \n",
    "        # будем хранить пары (модель, фичи)\n",
    "        self.models = []\n",
    "        self.features_ids = []\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.criterion = loss_dict[loss]\n",
    "        self.lr = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "        \n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, base_model=DecisionTreeRegressor, init_model=None):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        if init_model == None:\n",
    "            # выбираем случайную константу\n",
    "            #const_pred = y[np.random.choice(y, 1)]\n",
    "            const_pred = stats.mode(y, keepdims=True)[0]\n",
    "            # готовим таргет для бустинга\n",
    "            y_boosting = (-1) * self.criterion.grad(y, const_pred)\n",
    "            # сохраняем\n",
    "            self.models.append(const_pred)\n",
    "            if self.random_state == None:\n",
    "                self.models += [base_model(*self.args, **self.kwargs) for i in range(1, self.n_estimators)]\n",
    "            else:\n",
    "                self.models += [base_model(random_state = self.random_state, *self.args, **self.kwargs) for i in range(1, self.n_estimators)]\n",
    "            self.features_ids.append(None)\n",
    "            \n",
    "        else:\n",
    "            # инициализируем выданную модель\n",
    "            if self.random_state == None:\n",
    "                start_model = init_model(*self.args, **self.kwargs)\n",
    "            else:\n",
    "                start_model = init_model(random_state = self.random_state, *self.args, **self.kwargs)\n",
    "            # отбираем фичи и объекты\n",
    "            features_id = np.random.choice(X.shape[1], int(self.colsample * X.shape[1]))\n",
    "            samples_id = np.random.choice(X.shape[0], int(self.subsample * X.shape[0]))\n",
    "            # берем подвыборку\n",
    "            X_train = X[samples_id][:, features_id]\n",
    "            y_train = y[samples_id]\n",
    "            # учим на подвыборке\n",
    "            start_model.fit(X_train, y_train)\n",
    "            # сохраняем\n",
    "            self.models.append(start_model)\n",
    "            if self.random_state == None:\n",
    "                self.models += [base_model(*self.args, **self.kwargs) for i in range(1, self.n_estimators)]\n",
    "            else:\n",
    "                self.models += [base_model(random_state = self.random_state, *self.args, **self.kwargs) for i in range(1, self.n_estimators)]\n",
    "            self.features_ids.append(features_id)\n",
    "            # подготовим таргет для следующих моделей\n",
    "            y_boosting = (-1) * self.criterion.grad(self.predict(X), y)\n",
    "            # таргеты, которые не затронуло оставим прежними\n",
    "            #y_boosting = y.copy()\n",
    "            #y_boosting[samples_id] = boosting_target\n",
    "            \n",
    "        for i, boosting_model in enumerate(self.models[1:]):\n",
    "            # инициализация новой модели\n",
    "            # boosting_model = base_model(*self.args, **self.kwargs)\n",
    "            # выбираем фичи и объекты\n",
    "            features_id = np.random.choice(X.shape[1], int(self.colsample * X.shape[1]))\n",
    "            samples_id = np.random.choice(X.shape[0], int(self.subsample * X.shape[0]))\n",
    "            # берем подвыборку\n",
    "            X_train = X[samples_id][:, features_id]\n",
    "            y_train = y_boosting[samples_id]\n",
    "            # учим\n",
    "            boosting_model.fit(X_train, y_train)\n",
    "            # сохраняем\n",
    "            #self.models.append((boosting_model, features_id))\n",
    "            self.features_ids.append(features_id)\n",
    "            #print(f\"Train {i} loss: {self.criterion.loss(self.predict(X), y)} | accuracy: {accuracy_score(y_pred=np.around(self.predict(X)).astype(int), y_true=y)}\")\n",
    "            \n",
    "            # готовим новые таргеты\n",
    "            y_boosting = (-1) * self.criterion.grad(self.predict(X), y)\n",
    "            # корректируем все таргеты\n",
    "            #y_boosting[samples_id] = boosting_target\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Ваш код здесь\n",
    "        if self.features_ids[0] is None:\n",
    "            # константный прогноз\n",
    "            pred = self.models[0]\n",
    "        else:\n",
    "            # достаем модель\n",
    "            model = self.models[0]\n",
    "            # достаем фичи, на которых она работает\n",
    "            features_id = self.features_ids[0]\n",
    "            # делаем предсказание\n",
    "            pred = model.predict(X[:, features_id])\n",
    "            \n",
    "        for i in range(1, len(self.features_ids)):\n",
    "            # достаем модель\n",
    "            model = self.models[i]\n",
    "            # достаем фичи, на которых она работает\n",
    "            features_id = self.features_ids[i]\n",
    "            # смещаем предсказание\n",
    "            pred = pred + self.lr * model.predict(X[:, features_id])\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=100, \n",
    "                                      colsample=0.7, \n",
    "                                      subsample=0.7,\n",
    "                                      random_state=123\n",
    "                                     )\n",
    "clf = GradientBoostingClassifier(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "n_estimators_range = [i for i in range(100, 300, 50)] \n",
    "loss_plot = {\n",
    "    'mse' : [],\n",
    "    'exp' : [],\n",
    "    'log' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mse | accuracy: 0.864 n: 250: 100%|█████████████████████████████████| 4/4 [15:29<00:00, 232.44s/it]\n",
      "exp | accuracy: 0.578 n: 250: 100%|█████████████████████████████████| 4/4 [08:21<00:00, 125.50s/it]\n",
      "log | accuracy: 0.578 n: 250: 100%|█████████████████████████████████| 4/4 [07:50<00:00, 117.53s/it]\n"
     ]
    }
   ],
   "source": [
    "for loss_type in loss_plot.keys():\n",
    "    with tqdm(n_estimators_range) as t:\n",
    "        for n_estimators in t:\n",
    "            if loss_type == 'mse':\n",
    "                my_clf = MyGradientBoostingClassifier(loss=loss_type, \n",
    "                                                      learning_rate=1e-1, \n",
    "                                                      n_estimators=n_estimators, \n",
    "                                                      colsample=1.0, \n",
    "                                                      subsample=1.0,\n",
    "                                                      random_state=123)\n",
    "            else:\n",
    "                my_clf = MyGradientBoostingClassifier(loss=loss_type, \n",
    "                                                      learning_rate=1e-3, \n",
    "                                                      n_estimators=n_estimators, \n",
    "                                                      colsample=1.0, \n",
    "                                                      subsample=1.0,\n",
    "                                                      random_state=123)\n",
    "            my_clf.fit(X_train, y_train)\n",
    "            pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "            score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "            t.set_description(f\"{loss_type} | accuracy: {np.round(score, 3)} n: {n_estimators}\")\n",
    "            loss_plot[loss_type].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjqklEQVR4nO3dfXyV9X3/8dcn9yQh4SYB5M4EBJEbizahrbRW51TUTmpXFWofP123atfZuXbrxM3WzvbR2tptbX9z/dV2zK62lWpdSztX8Y46XBVC1QpBCwTUAEq4CSEJIXef3x/XlZMr4QocIIdzQt7Px+M8cq7vdV3nfHL05M33+71uzN0RERHpLyvdBYiISGZSQIiISCwFhIiIxFJAiIhILAWEiIjEykl3AYOlrKzMKyoq0l2GiMiQsn79+j3uXh637rQJiIqKCmpqatJdhojIkGJmrw+0TkNMIiISSwEhIiKxFBAiIhJLASEiIrEUECIiEksBISIisRQQIiIS67Q5D+JEtXd288MXXmd0YR6jCnMTP0cV5lFSkIOZpbtEEZG0GPYBsb+1nX/4RW3suuwsY9SI3Ehw5DG6MDcRIKMTy3mMLgq2KR2RS0Fu9in+LUREBt+wD4jy4nx++7lL2d/aTmNrO/tbOmg81BE8b21nf2tHor1+fysbd3awv7Wdto7uAV9zRG52n+BIBMuI3l5Kb3uwrqQgl6ws9VZEJHMM+4DIyjLGFOUxpijvuPZr6+gKAqSlJ0yC4DhwqIP9LZFgaW1nV2NTYl33ADfwM4PSEblHDHX1BEhp+LNvex4j8tRbEZHUGPYBcaIKcrM5o3QEZ5SOSHqf7m7nYFtn2DNppzEMlf2tHRyIhExjawdvN7Xx2lsH2d/aTmt714CvmZ+TdUSgjIqESWkkZHraS0fkkpOt4xNE5OgUEKdQVpZRWphLaWEuFRQlvd/hzi4OtHZEAqRvmDRGhsI2726mMWzvHKi7ApQU5DC6KC+cY4kGSO/w16hoj6Yoj6K8bE3aiwwjCoghID8nm3El2YwrKUh6H3fn4OHOMFiicym9zxsP9YZO3Z5mGls6OHi4c8DXzM22PmGSCJCi/r2UniPBgvZc9VZEhiQFxGnKzCgpCCa/p4wpTHq/jq5uGls7OHAo7KW09B0Ka4wMj72+t5WX3myksbWD9q6BJ+2L83OOOIQ4OuTVf16ltDBXhxiLZAAFhPSRm51F+ch8ykfmJ72Pu9Pa3hUZ8oofCuv5+ea+1mDe5VDHgK+ZnWVMKClg9sQS5k4sZc7EEuZOKmV8Sb6CQ+QUUUDISTMzivJzKMrPYfLo5Pfr6vbgqK8wTBrDeZaeXsob+w6xcecBntz0Nh5Op4wtygtCY1IYGhNLmTqmUIcIi6SAAkLSJjvJQ4xbDneyaVcTG3c2sWHHATbubOK7z9YlJuGL83OYPbGEORNLmDOxlLmTSjirvFhHaomcJAWEZLyi/ByqKsZQVTEm0Xa4s4vNbzezcecBNuxoYuPOA/x47RuJExjzcrI4Z8JIZoeBMWdiKbMmjNRZ7nJa6u72lPSizX3gQyGHkqqqKtc9qYe3rm5n257mRGD09Dia2oIjs7KzjLPKi4OeRjhENXtiCSUFuWmuXOTY3J2Gg4fZ2tBC3Z5mtu4OftY1tDB1TCEP/tm7Tuh1zWy9u1fFrVMPQk4b2VnGWeNGcta4kXzwvElA8KWq338oERgbdzaxZsseHn1xR2K/M8cWJoanen4ezyS9yGBq6+hi+94W6hpaqGtoDgKhIQiC6GHoBblZVJYVc+7kUuZPGZWSWtSDkGFp98E2Nu5sojYyr/HGvtbE+vEl+cF8xsSSxDDVpFEjdASVDIqe3sCW8A9/XUMLWxuaqdvTTP3+Q0T/LJ9RWsC08iKmlxczrayIaeXFTB9XzBklBYMyrKQehEg/40YWMO7sAi4+e1yi7cChDmp3NkV6GwdY/druxPWzSkfkJuYzenoalWVFZOsIKhlAT29g6+6wF7AnDIKGFpojvYERudlUlhXxjsmj+NB5kxOBUFlWRFF++v5MqwchchSH2rt49a0mNuxsojYMjld3HUycGDgiNztxBNXciaXMnljCzPEjycvREVTDhbuz++BhtvYbDtra0MyOxr69gYmlBUwrL+7tEYQ/JwxSb+BEHK0HoYAQOU4dXd1s2d2cGJrauPMAtTubaAkvqpibbcwcPzJxct+ciSWcc0YJhXnqsA9lbR1dbNsTGQ4KA2HbniN7A9PKw6Gg8GcwNFSUkf8PpC0gzGwR8E0gG/ieu9/Tb/1U4PvAqHCbZe7+mJlVAJuA18JNn3f3TxztvRQQkk7d3c7r+1r7hMbGnU3sa2kHgsu5TysrSpyn0TNMNarw+C4zL6nl7rzddDj84x/2CPa0sHV3MzsP9O0NTBo1IgiCsiKmjytmWlnQI0hnb+BEpCUgzCwb+D1wKVAPrAOWunttZJv7gRfd/dtmNht4zN0rwoD4pbvPTfb9FBCSadydXQfaEoGxYUcwTLXzQFtim0mjRvTpacydVMq4kbqcSKodag97A/0OF61raE70BAEK88LeQFnfYaHKsszsDZyIdE1SLwC2uHtdWMRDwGIgen9PB0rC56XAzhTWI3JKmRkTR41g4qgRXDp7fKJ9X0t7nxP8Nu5sYlXt24n1ZcV5fSbC504qYeqYQoXGcXJ33mpq63O4aM8E8Y7GQ3227ekNXFs1pU8QTCgpGNafeyoDYhLwZmS5Huh/JscXgFVm9imgCPjDyLpKM3sRaALudPf/SWGtIqfMmKI83jejnPfNKE+0NYeXE+kZotqw4wDPbdmTuJzIyMTlRHqHqKaXF+lyIgS9gd4eQO/hotsaWmJ7A1UVo7mubEqfI4V0Z8Z46e4jLQUecPd/NLP3AD8ws7nALmCqu+81s3cCPzOzOe7eFN3ZzG4GbgaYOnXqqa5dZNAU5+dQXTGG6sjlRNo6uvj92wf7DFH9aO3ricuJ5OdkMeuM3iOo5kws4ezT9HIiPb2B6HBQXG/ADCaWBr2BqqoxiUni6eXFuhLwCUjlHMR7gC+4++Xh8h0A7v6VyDYbgUXu/ma4XAe8291393ut1cDfuPuAkwyag5DhoLOrm7o9LUcMUR2MXE5kxrjiPpdJnz2xhJFD5HIiPb2B/oeLbtvT0ufWu0V52UccLjqtTL2BE5GuOYh1wAwzqwR2AEuAj/Tb5g3gEuABMzsHKAAazKwc2OfuXWY2DZgB1KWwVpEhISc7i5njRzJz/EiuOS9oc3feDC+NviEMjGd/v4dHf9t7OZGKsYXBvEbkCKqy4vRcTqRn8j56uGjvkUK9E/hmPXMDxVRXjGH6uGKmh2cSqzdwaqQsINy908xuBR4nOIR1ubtvNLO7gRp3Xwn8NfBdM/s0wYT1Te7uZnYhcLeZdQDdwCfcfV+qahUZysyMqWMLmTq2kCvmnZFo393UFgTGjuAaVC/XN/Jfr+xKrJ9QUsDcScGlRHqOoJpYOniTsq3tnX2Ggur29PYKDnX09gaK83OYVl7EgsoxYW+g90ih03G4bCjRiXIiw0hja3t4OZFwXmNnE3UNzYnLiYwqzE0MTfVc8bZybNGAx/V3dzu7mtqCo4R2N4chEITCrn69gcmjRxxxuOj08mId1ptmuhaTiAAwqjCPC84q44KzyhJtre2dbNp1kNqeeY1dB1j+3DY6uoLUKMzLZnY4GX7W+JHsCS8rUReeRdy/NzC9vIh3TxvbewJZeREVY9UbGIoUECLDXGFeDu88czTvPLP3frHtnd1s3h0eQRUeevvw+npa27sSvYHp5cVBEIQ9genlRZSrN3BaUUCIyBHycrLCyexSqJoCBDdkequpjbFFeeoNDBMKCBFJSnaWMWnUiHSXIaeQTsMUEZFYCggREYmlgBARkVgKCBERiaWAEBGRWAoIERGJpYAQEZFYCggREYmlgBARkVgKCBERiaWAEBGRWAoIERGJpYAQEZFYCggREYmlgBARkVgKCBERiaWAEBGRWAoIERGJpYAQEZFYCggREYmV0oAws0Vm9pqZbTGzZTHrp5rZM2b2opn9zsyujKy7I9zvNTO7PJV1iojIkXJS9cJmlg3cB1wK1APrzGylu9dGNrsT+Im7f9vMZgOPARXh8yXAHGAi8KSZzXT3rlTVKyIifaWyB7EA2OLude7eDjwELO63jQMl4fNSYGf4fDHwkLsfdvdtwJbw9URE5BRJZUBMAt6MLNeHbVFfAD5qZvUEvYdPHce+mNnNZlZjZjUNDQ2DVbeIiJD+SeqlwAPuPhm4EviBmSVdk7vf7+5V7l5VXl6esiJFRIajlM1BADuAKZHlyWFb1J8CiwDc/TdmVgCUJbmviIikUCp7EOuAGWZWaWZ5BJPOK/tt8wZwCYCZnQMUAA3hdkvMLN/MKoEZwNoU1ioiIv2krAfh7p1mdivwOJANLHf3jWZ2N1Dj7iuBvwa+a2afJpiwvsndHdhoZj8BaoFO4C90BJOIyKllwd/joa+qqspramrSXYaIyJBiZuvdvSpuXbonqUVEJEMpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQkVirvSS0iMmR0dHRQX19PW1tbuktJiYKCAiZPnkxubm7S+yggRESA+vp6Ro4cSUVFBWaW7nIGlbuzd+9e6uvrqaysTHo/DTGJiABtbW2MHTv2tAsHADNj7Nixx907UkCIiIROx3DocSK/W1IBYWaPmtlVZqZAEREZJpL9g/+vwEeAzWZ2j5mdncKaREQkAyQVEO7+pLvfAJwPbAeeNLP/NbM/MbPkp8RFRGTISHrIyMzGAjcBfwa8CHyTIDCeSEllIiLDzPbt25k1axY33XQTM2fO5IYbbuDJJ59k4cKFzJgxg7Vr1/LrX/+a+fPnM3/+fM477zwOHjwIwL333kt1dTXnnnsud91116DUk9Rhrmb2n8DZwA+AP3L3XeGqFWZWc5T9FhEESTbwPXe/p9/6fwYuDhcLgXHuPipc1wW8Eq57w92vTuo3EhE5Sf/wi43U7mwa1NecPbGEu/5ozjG327JlCw8//DDLly+nurqaH/3oR6xZs4aVK1fy5S9/ma6uLu677z4WLlxIc3MzBQUFrFq1is2bN7N27Vrcnauvvppnn32WCy+88KRqTvY8iG+5+zNxK9y9Kq7dzLKB+4BLgXpgnZmtdPfayL6fjmz/KeC8yEsccvf5SdYnInJaqKysZN68eQDMmTOHSy65BDNj3rx5bN++nSVLlvCZz3yGG264gQ996ENMnjyZVatWsWrVKs47L/gT2tzczObNm09ZQMw2sxfdvRHAzEYDS939X4+yzwJgi7vXhfs8BCwGagfYfikwOP0iEZGTkMy/9FMlPz8/8TwrKyuxnJWVRWdnJ8uWLeOqq67iscceY+HChTz++OO4O3fccQe33HLLoNaS7BzEx3vCAcDd9wMfP8Y+k4A3I8v1YdsRzOxMoBJ4OtJcYGY1Zva8mX1wgP1uDrepaWhoOPZvISIyxG3dupV58+Zx++23U11dzauvvsrll1/O8uXLaW5uBmDHjh3s3r37pN8r2R5EtpmZuzskho/yTvrdey0BHnH3rkjbme6+w8ymAU+b2SvuvjW6k7vfD9wPUFVV5YNYj4hIRvrGN77BM888Q1ZWFnPmzOGKK64gPz+fTZs28Z73vAeA4uJiHnzwQcaNG3dS75VsQPyKYEL6O+HyLWHb0ewApkSWJ4dtcZYAfxFtcPcd4c86M1tNMD+x9chdRURODxUVFWzYsCGx/MADDwy4rr/bbruN2267bVDrSXaI6XbgGeDPw8dTwN8eY591wAwzqzSzPIIQWNl/IzObBYwGfhNpG21m+eHzMmAhA89diIhICiTVg3D3buDb4SMp7t5pZrcCjxMc5rrc3Tea2d1Ajbv3hMUS4KGe4avQOcB3zKybIMTuiR79JCIiqZfseRAzgK8As4GCnnZ3n3a0/dz9MeCxfm2f77f8hZj9/heYl0xtIiKSGskOMf07Qe+hk+DEtv8AHkxVUSIikn7JBsQId38KMHd/PfxX/1WpK0tERNIt2aOYDoeX+t4czivsAIpTV5aIiKRbsj2I2wiulfSXwDuBjwI3pqooERFJv2P2IMKT4q53978BmoE/SXlVIiKSdsfsQYRnN7/3FNQiIjLsPfjggyxYsID58+dzyy238MILL3DuuefS1tZGS0sLc+bMYcOGDaxevZoLL7yQq666irPPPptPfOITdHd3D2otyc5BvGhmK4GHgZaeRnd/dFCrERHJBP+9DN565djbHY8J8+CKe466yaZNm1ixYgXPPfccubm5fPKTn+S1117j6quv5s477+TQoUN89KMfZe7cuaxevZq1a9dSW1vLmWeeyaJFi3j00Uf58Ic/PGglJxsQBcBe4A8ibQ4oIEREBslTTz3F+vXrqa6uBuDQoUOMGzeOz3/+81RXV1NQUMC3vvWtxPYLFixg2rTgdLSlS5eyZs2aUx8Q7q55BxEZPo7xL/1UcXduvPFGvvKVr/Rp37VrF83NzXR0dNDW1kZRUREAZtZnu/7LJyupo5jM7N/NbHn/x6BWIiIyzF1yySU88sgjiUt179u3j9dff51bbrmFL37xi9xwww3cfvvtie3Xrl3Ltm3b6O7uZsWKFbz3vYM7XZzsENMvI88LgGuAnYNaiYjIMDd79my+9KUvcdlll9Hd3U1ubi6LFy8mNzeXj3zkI3R1dXHBBRfw9NNPk5WVRXV1Nbfeeitbtmzh4osv5pprrhnUepIdYvppdNnMfgysGdRKRESE66+/nuuvvz52XXZ2Ni+88AIAq1evpqSkhF/+8pex2w6GZE+U628GcHJ3ohARkYyW7NVcDxIctdTjLYJ7RIiISBpcdNFFXHTRRSl9j2SHmEamtAoREck4yR7FdI2ZlUaWR5nZB1NWlYiIpF2ycxB3ufuBngV3bwTuSklFIiKSEZINiLjtkj1EVkREhqBkA6LGzP7JzKaHj38C1qeyMBGR4aa4OLNus5NsQHwKaAdWAA8BbcBfpKooERFJv6QCwt1b3H2Zu1e5e7W7/527txx7TxEROV7uzmc/+1nmzp3LvHnzWLFiBQDd3d188pOfZNasWVx66aVceeWVPPLIIymrI9nzIJ4Arg0npzGz0cBD7n55yioTEUmTr679Kq/ue3VQX3PWmFncviC508ceffRRXnrpJV5++WX27NlDdXU1F154Ic899xzbt2+ntraW3bt3c8455/Cxj31sUOuMSnaIqawnHADcfT86k1pEJCXWrFnD0qVLyc7OZvz48bz//e9n3bp1rFmzhmuvvZasrCwmTJjAxRdfnNI6kj0SqdvMprr7GwBmVkHfM6tjmdki4JtANvA9d7+n3/p/Bnp+w0JgnLuPCtfdCNwZrvuSu38/yVpFRE5Ksv/SP90l24P4e2CNmf3AzB4Efg3ccbQdwntZ3wdcAcwGlprZ7Og27v5pd5/v7vOB/0t4AyIzG0NwnsW7gAXAXeGwlojIae9973sfK1asoKuri4aGBp599lkWLFjAwoUL+elPf0p3dzdvv/02q1evTmkdyV5q41dmVgXcDLwI/Aw4dIzdFgBb3L0OwMweAhYDtQNsv5Tek+8uB55w933hvk8Ai4AfJ1OviMhQds011/Cb3/yGd7zjHZgZX/va15gwYQJ//Md/zFNPPcXs2bOZMmUK559/PqWlpcd+wROU7CT1nwG3AZOBl4B3A7+h7y1I+5sEvBlZrifoEcS9/plAJfD0UfadFLPfzQShxdSpU4/9i4iIZLDm5mYguDPcvffey7333ttnfVZWFl//+tcpLi5m7969LFiwgHnz5qWsnmTnIG4DqoHn3f1iM5sFfHkQ61gCPOLuXcezk7vfD9wPUFVVdcw5ERGRoe4DH/gAjY2NtLe387nPfY4JEyak7L2SDYg2d28zM8ws391fNbOzj7HPDmBKZHly2BZnCX1PvNsBXNRv39VJ1ioictpK9bxDVLKT1PVmNopg7uEJM/s58Pox9lkHzDCzSjPLIwiBlf03CnsjowmGrHo8DlxmZqPDyenLwjYRkZRxP30HIk7kd0t2krrnRqdfMLNngFLgV8fYp9PMbiX4w54NLHf3jWZ2N1Dj7j1hsYTgpDuP7LvPzL5IEDIAd/dMWIuIpEJBQQF79+5l7NixmFm6yxlU7s7evXspKCg4rv3sdEnMqqoqr6mpSXcZIjJEdXR0UF9fT1tbW7pLSYmCggImT55Mbm5un3YzW+/uVXH76JLdIiJAbm4ulZWV6S4joyQ7ByEiIsOMAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQkVkoDwswWmdlrZrbFzJYNsM11ZlZrZhvN7EeR9i4zeyl8rExlnSIicqScVL2wmWUD9wGXAvXAOjNb6e61kW1mAHcAC919v5mNi7zEIXefn6r6RETk6FLZg1gAbHH3OndvBx4CFvfb5uPAfe6+H8Ddd6ewHhEROQ6pDIhJwJuR5fqwLWomMNPMnjOz581sUWRdgZnVhO0fjHsDM7s53KamoaFhUIsXERnuUjbEdBzvPwO4CJgMPGtm89y9ETjT3XeY2TTgaTN7xd23Rnd29/uB+wGqqqr8lFYuInKaS2UPYgcwJbI8OWyLqgdWunuHu28Dfk8QGLj7jvBnHbAaOC+FtYqISD+pDIh1wAwzqzSzPGAJ0P9opJ8R9B4wszKCIac6MxttZvmR9oVALSIicsqkbIjJ3TvN7FbgcSAbWO7uG83sbqDG3VeG6y4zs1qgC/isu+81swuA75hZN0GI3RM9+klERFLP3E+PofuqqiqvqalJdxkiIkOKma1396q4dTqTWkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJCRERiKSBERCSWAkJERGKlNCDMbJGZvWZmW8xs2QDbXGdmtWa20cx+FGm/0cw2h48bU1mniIgcKSdVL2xm2cB9wKVAPbDOzFa6e21kmxnAHcBCd99vZuPC9jHAXUAV4MD6cN/9qapXRET6SmUPYgGwxd3r3L0deAhY3G+bjwP39fzhd/fdYfvlwBPuvi9c9wSwKIW1iohIP6kMiEnAm5Hl+rAtaiYw08yeM7PnzWzRcewrIiIplLIhpuN4/xnARcBk4Fkzm5fszmZ2M3AzwNSpU1NRn4jIsJXKHsQOYEpkeXLYFlUPrHT3DnffBvyeIDCS2Rd3v9/dq9y9qry8fFCLFxEZ7lIZEOuAGWZWaWZ5wBJgZb9tfkbQe8DMygiGnOqAx4HLzGy0mY0GLgvbRETkFEnZEJO7d5rZrQR/2LOB5e6+0czuBmrcfSW9QVALdAGfdfe9AGb2RYKQAbjb3felqlYRETmSuXu6axgUVVVVXlNTk+4yRESGFDNb7+5Vcet0JrWIiMRSQIiISCwFhIiIxFJAiIhILAWEiIjEUkCIiEgsBYSIiMRSQIiISKx0X6wvI3z1J3/Eq6270l2GiMgJmVV4Brdf94tBf131IEREJJZ6EJCS5BURGerUgxARkVgKCBERiaWAEBGRWAoIERGJpYAQEZFYCggREYmlgBARkVgKCBERiXXa3JPazBqA10/iJcqAPYNUTipken2Q+TVmen2gGgdDptcHmVXjme5eHrfitAmIk2VmNQPduDsTZHp9kPk1Znp9oBoHQ6bXB0OjRtAQk4iIDEABISIisRQQve5PdwHHkOn1QebXmOn1gWocDJleHwyNGjUHISIi8dSDEBGRWAoIERGJNSwCwsyWm9luM9sQaRtjZk+Y2ebw5+iw3czsW2a2xcx+Z2bnp7HGe83s1bCO/zSzUZF1d4Q1vmZml6erxsi6vzYzN7OycPmUf44D1Wdmnwo/x41m9rVIe0Z8hmY238yeN7OXzKzGzBaE7en4DKeY2TNmVht+XreF7RnxfTlKfRnzXRmoxsj6tH9Xkubup/0DuBA4H9gQafsasCx8vgz4avj8SuC/AQPeDbyQxhovA3LC51+N1DgbeBnIByqBrUB2OmoM26cAjxOcqFiWrs9xgM/wYuBJID9cHpdpnyGwCrgi8rmtTuNneAZwfvh8JPD78LPKiO/LUerLmO/KQDWGyxnxXUn2MSx6EO7+LLCvX/Ni4Pvh8+8DH4y0/4cHngdGmdkZ6ajR3Ve5e2e4+DwwOVLjQ+5+2N23AVuABemoMfTPwN8C0SMeTvnnOEB9fw7c4+6Hw212R+rLlM/QgZLweSmwM1Ljqf4Md7n7b8PnB4FNwCQy5PsyUH2Z9F05ymcIGfJdSdawCIgBjHf3XeHzt4Dx4fNJwJuR7erp/Y+bTh8j+FcGZFCNZrYY2OHuL/dblSk1zgTeZ2YvmNmvzaw6bM+U+gD+CrjXzN4Evg7cEbantUYzqwDOA14gA78v/eqLypjvSrTGIfBdOUJOugvIBO7uZpaxx/ua2d8DncAP011LlJkVAn9H0L3PVDnAGIKuezXwEzOblt6SjvDnwKfd/admdh3wb8AfprMgMysGfgr8lbs3mVliXSZ8X/rXF2nPmO9KtEaCmjL9u3KE4dyDeLunGxf+7Bl62EEwTthjctiWFmZ2E/AB4AYPByzJnBqnE4zrvmxm28M6fmtmE8icGuuBR8Pu+1qgm+BCaZlSH8CNwKPh84fpHQJJS41mlkvwh+2H7t5TV8Z8XwaoL6O+KzE1DoXvyhGGc0CsJPhiEv78eaT9/4RHFrwbOBDpWp9SZraIYLzyandvjaxaCSwxs3wzqwRmAGtPdX3u/oq7j3P3CnevIPhjfL67v0XmfI4/I5ioxsxmAnkEV9HMiM8wtBN4f/j8D4DN4fNT/hla0FX4N2CTu/9TZFVGfF8Gqi+TvitxNQ6R78qR0j1LfioewI+BXUAHwX+YPwXGAk8RfBmfBMaE2xpwH8HRDq8AVWmscQvB2ORL4eP/Rbb/+7DG1wiPgElHjf3Wb6f3yIxT/jkO8BnmAQ8CG4DfAn+QaZ8h8F5gPcHRNi8A70zjZ/heggnU30X+v7syU74vR6kvY74rA9XYb5u0fleSfehSGyIiEms4DzGJiMhRKCBERCSWAkJERGIpIEREJJYCQkREYikgREQklgJC5CRYcKnuKyPLV5vZskF67b8KL2cikhY6D0LkJISXd6hy91tT8Nrbw9fecxz7ZLt712DXIsOTehAyLJhZhZltMrPvhjdxWWVmIwbYdrqZ/crM1pvZ/5jZrLD9WjPbYGYvm9mzZpYH3A1cb8HNfq43s5vM7F/C7R8ws29bcDOgOjO7yIIbBm0yswci7/dtC24UtNHM/iFs+0tgIvCMmT0Tti01s1fCGr4a2b/ZzP7RzF4G3mNm91hws5rfmdnXU/OJyrCQ7lO59dDjVDyACoIras4Pl38CfHSAbZ8CZoTP3wU8HT5/heDeAwCjwp83Af8S2TexDDwAPERwKYXFQBMwj+AfZusjtfRctiIbWA2cGy5vp/dyDBOBN4BygivUPg18MFznwHXh87EEl5SwaJ166HEiD/UgZDjZ5u4vhc/XE4RGH+Elmi8AHjazl4DvENwhDOA54AEz+zjBH/Nk/MLdnSBc3vbgom3dwMbI+19nZr8FXgTmENwFrb9qgjvNNXhwY5wfEtydDqCL4MqhAAeANuDfzOxDQOsRrySSJN0PQoaTw5HnXUDcEFMW0Oju8/uvcPdPmNm7gKuA9Wb2zuN4z+5+798N5IRXGP0boNrd94dDTwVJvG5Um4fzDu7eacE9rS8BPgzcSnCFWJHjph6ESIQHN5/ZZmbXQuKG8u8In0939xfc/fNAA8E1/A8S3Hf4RJUALcABMxsPXBFZF33ttcD7zazMzLKBpcCv+79Y2AMqdffHgE8D7ziJ2mSYUw9C5Eg3AN82szuBXIJ5hJcJbgs6g2BO4amw7Q1gWTgc9ZXjfSN3f9nMXgReJbhc9XOR1fcDvzKzne5+cXj47DPh+/+Xu//8yFdkJPBzMysIt/vM8dYk0kOHuYqISCwNMYmISCwNMcmwZWb3AQv7NX/T3f89HfWIZBoNMYmISCwNMYmISCwFhIiIxFJAiIhILAWEiIjE+v+HQqCUAO9SJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "for loss_type in loss_plot.keys():\n",
    "    plt.plot(n_estimators_range, loss_plot[loss_type], label=loss_type)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "for loss_type in list(loss_plot.keys())[1:]:\n",
    "    plt.plot(n_estimators_range, loss_plot[loss_type], label=loss_type)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как пел Виктор Цой: \"Странное дело...\". Что имеем? MSE стабильней всех остальных, подберем параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.8, \n",
    "                                      subsample=0.8,\n",
    "                                      random_state=123)\n",
    "\n",
    "my_clf.fit(X_train, y_train)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По итогу буду пробовать брать 150 итераций и отбирать по 80% случайных фичей и объектов выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.8, \n",
    "                                      subsample=0.8,\n",
    "                                      random_state=123)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=RandomForestClassifier)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting & bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 10/10 [09:44<00:00, 58.41s/it]\n"
     ]
    }
   ],
   "source": [
    "n_boostings = 10\n",
    "\n",
    "pred = []\n",
    "bootstrap_size = X.shape[0]\n",
    "bagging_ensemble = []\n",
    "\n",
    "for i in tqdm(range(n_boostings)):\n",
    "    bootstrap_idx = np.random.choice(range(X_train.shape[0]), replace=True, size=bootstrap_size)\n",
    "    X_bootstrap, y_bootstrap = X_train[bootstrap_idx], y_train[bootstrap_idx]\n",
    "    \n",
    "    my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=100, \n",
    "                                      colsample=0.8, \n",
    "                                      subsample=0.8,\n",
    "                                      random_state=123)\n",
    "\n",
    "    my_clf.fit(X_bootstrap, y_bootstrap)\n",
    "    bagging_ensemble.append(my_clf)\n",
    "    pred.append(my_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy| mean: 0.8876 mode: 0.88485 median: 0.88485\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(pred)\n",
    "\n",
    "pred_mean = np.mean(pred, axis=0)\n",
    "pred_mode = stats.mode(np.round(pred).astype(int), keepdims=False)[0]\n",
    "pred_median = np.median(np.round(pred).astype(int), axis=0)\n",
    "\n",
    "score_mean = accuracy_score(y_pred=np.round(pred_mean).astype(int), y_true=y_test)\n",
    "score_mode = accuracy_score(y_pred=np.round(pred_mode).astype(int), y_true=y_test)\n",
    "score_median = accuracy_score(y_pred=np.round(pred_median).astype(int), y_true=y_test)\n",
    "\n",
    "print(f\"Accuracy| mean: {np.round(score_mean, 5)} mode: {np.round(score_mode, 5)} median: {np.round(score_median, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идею с бэгингом я сперва не особо воспринял, но потом решил сделать выборку размером с весь датасет. При этом еще вывел разные варианты аггрегации предиктов. Результаты получились интересные =)\\\n",
    "Инциализация случайным лесом - интересная тема. Получается на старте мы как бы пытаемся выбить хороший скор, а потом дообучаем бустингом, красивая стратегия.\\\n",
    "\n",
    "Overall, результаты получились повыше. У бэггинга за счет комбинации моделей (ошибка становится маловероятным событием, ибо ошибиться должны несколько моделей разом), у инициализации прирост за счет хорошего начального приближения случайным лесом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8645026\n"
     ]
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.8, \n",
    "                                      subsample=0.8,\n",
    "                                      random_state=123)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=RandomForestClassifier)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7703488\n"
     ]
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.8, \n",
    "                                      subsample=0.8,\n",
    "                                      random_state=123)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=DecisionTreeRegressor)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8661176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.8, \n",
    "                                      subsample=0.8)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=LinearRegression)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8465762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.7, \n",
    "                                      subsample=0.7)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=Ridge)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8617571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.7, \n",
    "                                      subsample=0.7)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=LogisticRegression)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8439922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "my_clf = MyGradientBoostingClassifier(loss='mse', \n",
    "                                      learning_rate=1e-1, \n",
    "                                      n_estimators=150, \n",
    "                                      colsample=0.7, \n",
    "                                      subsample=0.7)\n",
    "\n",
    "my_clf.fit(X_train, y_train, init_model=ElasticNet)\n",
    "\n",
    "pred = np.around(my_clf.predict(X_test)).astype(int)\n",
    "score = accuracy_score(y_pred=np.round(my_clf.predict(X_test)).astype(int), y_true=y_test)\n",
    "print(f\"accuracy: {np.round(score, 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошее качество у RandomForest и LinearRegression. RandomForest в принципе классный алгоритм, а вот DecisionTree наверное слабее ансамбля и поэтому уступает. Регрессии клево себя показали ибо в задаче два класса. По итогу, победила модель с инициализацией через LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| RandomForest | 0.8645026 |\n",
    "| DecisionTree | 0.7703488 |\n",
    "| LinearRegression | 0.8661176 |\n",
    "| RidgeRegression | 0.8465762 |\n",
    "| LogisticRegression | 0.8617571 |\n",
    "| ElasticNet | 0.8439922 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все было клево, но вот почему у меня пошли странности с антиградиентом - загадка. Хотел бы получить фидбек что я сделал не так. Там же наверное кроется причина почему у меня точность моделей на log и exp лоссах была постоянной на уровне 0.578 =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
